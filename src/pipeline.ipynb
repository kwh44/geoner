{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "future-frequency",
   "metadata": {},
   "source": [
    "## Article web page processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "blocked-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html2text\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "informal-registration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_html(url: str): #data_folder_html_file_id):\n",
    "    \"\"\"\n",
    "    Given url of the webpage returns the readable text from the page\n",
    "    \"\"\"\n",
    "    #road_trip = open(\"../data/\" + str(data_folder_html_file_id) + \".html\", \"r\").read()\n",
    "    road_trip = requests.get(url).text\n",
    "    soup = BeautifulSoup(road_trip)\n",
    "    links = []\n",
    "    for link in soup.findAll('a'):\n",
    "        http_link = link.get('href')\n",
    "        if \"nationalgeographic\" in http_link:\n",
    "            continue\n",
    "        if not http_link.startswith(\"http\"):\n",
    "            continue\n",
    "        if len(http_link) > 5:\n",
    "            links.append(http_link)\n",
    "    \n",
    "    text_maker = html2text.HTML2Text()\n",
    "    text_maker.ignore_links = True\n",
    "    text_maker.ignore_images = True\n",
    "    road_trip_processed = text_maker.handle(road_trip)\n",
    "    \n",
    "    start = \"ShareTweetEmail\"\n",
    "    index = road_trip_processed.find(start)\n",
    "    road_trip_processed = road_trip_processed[index + len(start) + 1:]\n",
    "    \n",
    "    end = road_trip_processed.find(\"ShareTweetEmail\")\n",
    "    road_trip_processed = road_trip_processed[:end]\n",
    "    road_trip_processed = road_trip_processed.replace(\"\\n\\n\", \". \")\n",
    "    for i in [\"\\n\", \"[\", \"]\", \"*\", \"#\", \"_\", \"\\\\'\", '\\\\']:\n",
    "        road_trip_processed = road_trip_processed.replace(i, \" \")\n",
    "    road_trip_processed = road_trip_processed.replace('>', \", \")\n",
    "    for i in range(4): # 4 seems enough\n",
    "        road_trip_processed = road_trip_processed.replace(\"  \", \" \")\n",
    "    road_trip_processed = road_trip_processed.replace(\"..\", \".\")\n",
    "               \n",
    "    return road_trip_processed.strip(), links\n",
    "\n",
    "def get_text_and_places() -> dict:\n",
    "    with open('/home/geoner/data/articles.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    for i in data:\n",
    "        text, _ = text_from_html(data[i][\"article_url\"])\n",
    "        data[i][\"text\"] = text      \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "under-thirty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.09 s, sys: 54.8 ms, total: 1.15 s\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%time data = get_text_and_places()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-wedding",
   "metadata": {},
   "source": [
    "## Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "perfect-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "controversial-wrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_trf')\n",
    "def get_ne(txt: str) -> list:\n",
    "    ne_types = [\"GPE\", \"FAC\", \"LOC\", \"ORG\"]\n",
    "    ne = nlp(txt).ents\n",
    "    target_ne = [[str(ne[i]).strip(), ne[i]] for i in range(len(ne)) if ne[i].label_ in ne_types]\n",
    "    for i in range(len(target_ne)):\n",
    "        if target_ne[i][0].startswith(\"the\"):\n",
    "            target_ne[i][0] = target_ne[i][0][4:]\n",
    "        if target_ne[i][0].endswith(','):\n",
    "            target_ne[i][0] = target_ne[i][0][:-1]\n",
    "        target_ne[i][0] = target_ne[i][0].strip()\n",
    "        if target_ne[i][0].endswith('\\'s'):\n",
    "            target_ne[i][0] = target_ne[i][0][:-2]    \n",
    "        target_ne[i][0] = target_ne[i][0].strip()\n",
    "    places = list(set(i[0] for i in target_ne)) # removing duplicates as well\n",
    "    places_and_poi_processed = []\n",
    "    # case: route 66 museum -> oklahoma route 66 museum\n",
    "    for i in places:\n",
    "        if i.startswith(\"National Geographic\"):\n",
    "            continue\n",
    "        places_and_poi_processed.append(i)\n",
    "    places_and_poi = [i for i in target_ne if i[0] in places_and_poi_processed]\n",
    "    places_and_poi.sort(key=lambda x: x[1].start_char)\n",
    "    ne_index = {}\n",
    "    for i in range(len(places_and_poi)):\n",
    "        if ne_index.get(places_and_poi[i][0]):\n",
    "            ne_index[places_and_poi[i][0]].append(i)\n",
    "        else:\n",
    "            ne_index[places_and_poi[i][0]] = [i]\n",
    "    return places_and_poi_processed, places_and_poi, ne_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-singapore",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "banned-chair",
   "metadata": {},
   "source": [
    "## Gazetteer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "precious-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "def str_similarity_percent(a: str, b: str) -> float:\n",
    "    return round(difflib.SequenceMatcher(lambda x: x == \" \", a, b).ratio(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "joint-bronze",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20'000 credits daily limit per application (identified by the parameter 'username'),\n",
    "# the hourly limit is 1000 credits. A credit is a web service request hit for most services.\n",
    "# An exception is thrown when the limit is exceeded. \n",
    "geonames = {\n",
    "    \"exact_match\":\"http://secure.geonames.org/searchJSON?maxRows=40&username=kwh44&name_equals=\",\n",
    "    \"general_match\": \"http://secure.geonames.org/searchJSON?maxRows=30&username=kwh44&name=\"\n",
    "}\n",
    "# https://www.gisgraphy.com/documentation/user-guide.php#fulltextservice\n",
    "gisgraphy = {\n",
    "    \"exact_match\": \"https://services.gisgraphy.com/fulltext/fulltextsearch?&format=json&allwordsrequired=true&q=\",\n",
    "    \"general_match\": \"https://services.gisgraphy.com/fulltext/fulltextsearch?&format=json&q=\"\n",
    "}\n",
    "resource_name = \"\"\n",
    "dbpedia = {\n",
    "    \"resource_name_match\": \"https://lookup.dbpedia.org/api/search?&typeName=place&format=JSON_FULL&maxResults=3&query=\",\n",
    "    \"resource_rdf_get\": f\"https://dbpedia.org/data/{resource_name}.json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-reserve",
   "metadata": {},
   "source": [
    "## Existence Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "known-graduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import reverse_geocoder as rg\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "chemical-disclaimer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_by_lat_lng(lat, lng):\n",
    "    return rg.search((lat,lng))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fatal-behavior",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Place:\n",
    "    \n",
    "    def __init__(self, place_name, country, state, place_type, lat, long, link=None, population=0):\n",
    "        self.place_name = place_name\n",
    "        self.country = country\n",
    "        self.state = state\n",
    "        self.place_type = place_type\n",
    "        self.lat = float(lat)\n",
    "        self.long = float(long)\n",
    "        self.link = link\n",
    "        self.population = population\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Place name: {self.place_name}; State: {self.state}, \\\n",
    "Country: {self.country}, Place Type: {self.place_type}, Lat: {self.lat}, Long: {self.long}\"\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.place_name == other.place_name and self.country == other.country and \\\n",
    "        self.state == other.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "special-thickness",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# can be made faster by parallelizing\n",
    "def existence_check(extracted_places: list) -> tuple:\n",
    "    geonames_search = dict()\n",
    "    gisgraphy_search = dict()\n",
    "    dbpedia_search = dict()\n",
    "    for i in extracted_places:\n",
    "        print(i)\n",
    "        # try dbpedia, maybe there is a wikipedia page for the place extracted\n",
    "        #print(\"DBpedia search\")\n",
    "        # dbpedia resource search\n",
    "        dbpedia_lookup = requests.get(dbpedia[\"resource_name_match\"] + i).json()\n",
    "        for result in dbpedia_lookup[\"docs\"]:\n",
    "            result_name = result[\"label\"][0][\"value\"]\n",
    "            redirect_result_name = []\n",
    "            if result.get(\"redirectlabel\"):\n",
    "                redirect_result_name = [k[\"value\"] for k in result[\"redirectlabel\"]]\n",
    "            relevance = max(str_similarity_percent(unidecode.unidecode(r), i) for r in redirect_result_name + [result_name])\n",
    "            print(\"Result relevance is \", relevance)\n",
    "            if relevance >= 0.65:\n",
    "                print(\"Wikipedia page found\")\n",
    "                # do resourse rdf json get\n",
    "                rdf_json_url = result[\"resource\"][0][\"value\"].replace(\"resource\", \"data\") + \".json\"\n",
    "                print(rdf_json_url)\n",
    "                try:\n",
    "                    resource_rdf = requests.get(rdf_json_url, timeout=0.2).json()\n",
    "                except:\n",
    "                    print(\"dbpedia timeout\")\n",
    "                    continue\n",
    "                if dbpedia_search.get(i):\n",
    "                    dbpedia_search[i].append([result[\"resource\"][0][\"value\"], resource_rdf])\n",
    "                else:\n",
    "                    dbpedia_search[i] = [[result[\"resource\"][0][\"value\"], resource_rdf]]\n",
    "        # geonames exact place match search\n",
    "        geoname_exact_lookup = requests.get(geonames[\"exact_match\"] + i).json()\n",
    "        # print(geoname_exact_lookup)\n",
    "        results_num = geoname_exact_lookup[\"totalResultsCount\"]\n",
    "        if results_num >= 1:\n",
    "            geonames_search[i] = geoname_exact_lookup\n",
    "            print(\"Geonames exact search found match\")\n",
    "            continue\n",
    "        # gisgraphy exact place match search\n",
    "        gisgraphy_exact_lookup = requests.get(gisgraphy[\"exact_match\"] + i).json()\n",
    "        # print(gisgraphy_exact_lookup)\n",
    "        results_num = gisgraphy_exact_lookup[\"response\"][\"numFound\"]\n",
    "        if results_num >= 1:\n",
    "            # gisgaphy is not that good with exact match, might find lots of places that constain the query string,\n",
    "            # but those places would add more noise for region localization module\n",
    "            if results_num < 80:\n",
    "                gisgraphy_search[i] = gisgraphy_exact_lookup\n",
    "                print(\"Gisgraphy exact search found match\")\n",
    "                continue\n",
    "        print(\"exact place name search failed\")        \n",
    "        # exact place name search failed\n",
    "        # geonames partial place match search\n",
    "        geoname_general_lookup = requests.get(geonames[\"general_match\"] + i).json()\n",
    "        results_num = geoname_general_lookup[\"totalResultsCount\"]\n",
    "        if results_num >= 1:\n",
    "            geonames_search[i] = geoname_general_lookup\n",
    "            print(\"Geonames partial place match found\")\n",
    "            continue\n",
    "        # gisgraphy partial place match search\n",
    "        gisgraphy_general_lookup = requests.get(gisgraphy[\"general_match\"] + i).json()\n",
    "        results_num = gisgraphy_general_lookup[\"response\"][\"numFound\"]\n",
    "        if results_num >= 1:\n",
    "            if results_num < 80:\n",
    "                gisgraphy_search[i] = gisgraphy_general_lookup\n",
    "                print(\"Geonames partial place match found\")\n",
    "                continue\n",
    "    return geonames_search, gisgraphy_search, dbpedia_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "right-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_lookup_results(lookup_response):\n",
    "    geonames_results, gisgraphy_results, dbpedia_rdf = lookup_response\n",
    "    unified_lookup_results = []\n",
    "    for key, value in geonames_results.items():\n",
    "        results = value['geonames']\n",
    "        for res in results:\n",
    "            if str_similarity_percent(key, res[\"toponymName\"]) < 0.75:\n",
    "                continue\n",
    "            # skip results that don't have required information\n",
    "            try:\n",
    "                lat = res[\"lat\"]\n",
    "                long = res[\"lng\"]\n",
    "                place_name = key # res[\"toponymName\"]\n",
    "                country = res[\"countryCode\"]\n",
    "                place_type = res[\"fcodeName\"]\n",
    "                population = res.get(\"population\", 0)\n",
    "                state = res[\"adminName1\"]\n",
    "                unified_lookup_results.append(Place(place_name, country, state, place_type, lat, long, population=population))\n",
    "            except:\n",
    "                continue\n",
    "    for key, value in gisgraphy_results.items():\n",
    "        num = len(value[\"response\"][\"docs\"])\n",
    "        for res in value[\"response\"][\"docs\"]:\n",
    "            if str_similarity_percent(key, res['name']) < 0.75:\n",
    "                continue\n",
    "            try:\n",
    "                lat = res[\"lat\"]\n",
    "                long = res[\"lng\"]\n",
    "                place_name = key\n",
    "                country = res[\"country_code\"]\n",
    "                place_type = res[\"placetype\"]\n",
    "                population = res.get(\"population\", 0)\n",
    "                state = get_state_by_lat_lng(lat, long)[\"admin1\"]\n",
    "                unified_lookup_results.append(Place(place_name, country, state, place_type, lat, long, population=population))\n",
    "            except:\n",
    "                continue\n",
    "    for key, value in dbpedia_rdf.items():\n",
    "        for result in value:\n",
    "            try:\n",
    "                rdf = result[1][result[0]]\n",
    "                long = rdf[\"http://www.w3.org/2003/01/geo/wgs84_pos#long\"][0][\"value\"]\n",
    "                lat = rdf[\"http://www.w3.org/2003/01/geo/wgs84_pos#lat\"][0][\"value\"]\n",
    "                place_type = \"Attraction\"\n",
    "                place_name = key\n",
    "                country_admin1 = get_state_by_lat_lng(lat, long)\n",
    "                state = country_admin1[\"admin1\"]\n",
    "                country = country_admin1[\"cc\"]\n",
    "                unified_lookup_results.append(Place(place_name, country, state, place_type, lat, long))\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    return unified_lookup_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-somerset",
   "metadata": {},
   "source": [
    "## Region localization and noise removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "rough-optics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_country(geo_points):\n",
    "    places = geo_points\n",
    "    countries = dict()\n",
    "    for i in places:\n",
    "        if countries.get(i.country):\n",
    "            countries[i.country] +=1\n",
    "        else:\n",
    "            countries[i.country] = 1\n",
    "    travel_country = max(countries, key=lambda x: countries[x])\n",
    "    travel_country_places = list(filter(lambda i: i.country == travel_country, places))\n",
    "    place_name_geocoding = dict()\n",
    "    for place in travel_country_places:\n",
    "        if place_name_geocoding.get(place.place_name):\n",
    "            place_name_geocoding[place.place_name].append(place)\n",
    "        else:\n",
    "            place_name_geocoding[place.place_name] = [place]\n",
    "    states = [i.state for i in travel_country_places if i.place_type == \"first-order administrative division\"]\n",
    "    return place_name_geocoding, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adjacent-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_and_leave_states(place_name_geocoding, states):\n",
    "    for i in place_name_geocoding:\n",
    "        possible_geolocation_num = len(place_name_geocoding[i]) \n",
    "        if possible_geolocation_num <= 1:\n",
    "            continue\n",
    "        filtered_geolocations = []\n",
    "        # state names are treated as first-order administrative division\n",
    "        for j in place_name_geocoding[i]:\n",
    "            if j.place_type == \"first-order administrative division\":\n",
    "                filtered_geolocations.append(j)\n",
    "                break\n",
    "        if filtered_geolocations:\n",
    "            place_name_geocoding[i] = filtered_geolocations\n",
    "            continue\n",
    "        filtered_geolocations = []    \n",
    "        # remove duplicate geolocations that come from different\n",
    "        # gazetteers and differ slightly in coordinates\n",
    "        for j in place_name_geocoding[i]:\n",
    "            if j.state not in states:\n",
    "                continue\n",
    "            duplicate = False\n",
    "            for q in place_name_geocoding[i]:\n",
    "                if j.state == q.state and abs(j.lat - q.lat) < 1.0 and abs(j.long - q.long) < 1.0:\n",
    "                    duplicate = True\n",
    "            if not duplicate or j not in filtered_geolocations:\n",
    "                filtered_geolocations.append(j)\n",
    "        place_name_geocoding[i] = filtered_geolocations\n",
    "        # remove geolocations that are located on state borders,\n",
    "        # that point to the same place, but in on gazetteer it on opposing sides of the border\n",
    "        filtered_geolocations = []    \n",
    "        for j in place_name_geocoding[i]:\n",
    "            duplicate = False\n",
    "            for q in filtered_geolocations:\n",
    "                if j != q and abs(j.lat - q.lat) < 1.0 and abs(j.long - q.long) < 1.0:\n",
    "                    duplicate = True\n",
    "            if not duplicate:\n",
    "                filtered_geolocations.append(j)\n",
    "        place_name_geocoding[i] = filtered_geolocations\n",
    "    return place_name_geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "exact-opinion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def located_based_on_neighbours(place_name_geocoding, ne_index, places_and_poi):\n",
    "    for i in place_name_geocoding:    \n",
    "        # for places that have two or more possible geolocations\n",
    "        # look at left and right target token in the text to \n",
    "        # decide which geolocation to choose\n",
    "        neighbours_state = []\n",
    "        if len(place_name_geocoding[i]) > 1:\n",
    "            first_mention = ne_index[i][0]\n",
    "            try:\n",
    "                right_neighbour = places_and_poi[first_mention + 1]\n",
    "                dist = abs(right_neighbour[1].start_char - places_and_poi[first_mention][1].start_char)\n",
    "                if dist > 800:\n",
    "                    raise ValueError(\"neighbour too far away\")\n",
    "                right_neighbour_name = right_neighbour[0]\n",
    "                if len(place_name_geocoding[right_neighbour_name]) == 1:\n",
    "                    neighbours_state.append([place_name_geocoding[right_neighbour_name][0].state, dist])\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                left_neighbour = places_and_poi[first_mention-1]\n",
    "                dist = abs(left_neighbour[1].start_char - places_and_poi[first_mention][1].start_char)\n",
    "                if dist > 800:\n",
    "                    raise ValueError(\"neighbour too far away\")\n",
    "                left_neighbour_name = left_neighbour[0]\n",
    "                if len(place_name_geocoding[left_neighbour_name]) == 1:\n",
    "                    neighbours_state.append([place_name_geocoding[left_neighbour_name][0].state, dist])\n",
    "            except:\n",
    "                pass\n",
    "        if neighbours_state:\n",
    "            filtered_geolocations = []\n",
    "            closest_neighbour_with_exact_geolocation = min(neighbours_state, key=lambda x: x[1])\n",
    "            filtered_state = closest_neighbour_with_exact_geolocation[0]\n",
    "            for j in place_name_geocoding[i]:\n",
    "                if j.state == filtered_state:\n",
    "                    filtered_geolocations.append(j)\n",
    "                    break\n",
    "            if filtered_geolocations:\n",
    "                place_name_geocoding[i] = filtered_geolocations\n",
    "            else:\n",
    "                # try assign target place to more distant neighbour region\n",
    "                other_neighbour = max(neighbours_state, key=lambda x: x[1])\n",
    "                filtered_state = other_neighbour[0]\n",
    "                for j in place_name_geocoding[i]:\n",
    "                    if j.state == filtered_state:\n",
    "                        filtered_geolocations.append(j)\n",
    "                        break\n",
    "                if filtered_geolocations:\n",
    "                    place_name_geocoding[i] = filtered_geolocations\n",
    "    return place_name_geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "former-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_based_on_population(place_name_geocoding):\n",
    "    for i in place_name_geocoding:\n",
    "        if len(place_name_geocoding[i]) > 1:\n",
    "            most_popular = max(place_name_geocoding[i], key=lambda x: x.population)\n",
    "            place_name_geocoding[i] = [most_popular]\n",
    "    return place_name_geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "american-vault",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_geolocations_not_in_states_mentioned(place_name_geocoding, states):\n",
    "    for i in place_name_geocoding:\n",
    "        if len(place_name_geocoding[i]) == 1:\n",
    "            if place_name_geocoding[i][0].state not in states:\n",
    "                place_name_geocoding[i] = []\n",
    "    return place_name_geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "subject-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_geo_point(place_name_geocoding):\n",
    "    remove_keys = []\n",
    "    for i in place_name_geocoding:\n",
    "        if len(place_name_geocoding[i]) == 0:\n",
    "            remove_keys.append(i)\n",
    "    for i in remove_keys:\n",
    "        del place_name_geocoding[i]\n",
    "    return place_name_geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "comparative-ocean",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mere_state_names(place_name_geocoding):\n",
    "    for i in place_name_geocoding:\n",
    "        state_name = False\n",
    "        for j in place_name_geocoding[i]:\n",
    "            if j.place_type == \"first-order administrative division\":\n",
    "                state_name = True\n",
    "        if state_name:\n",
    "            place_name_geocoding[i] = []\n",
    "    return place_name_geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "distinct-constant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disambiguate(places_geocoded, ne_index, places_and_poi):\n",
    "    x, y = leave_one_country(places_geocoded)\n",
    "    x = remove_duplicates_and_leave_states(x, y)\n",
    "    x = remove_geolocations_not_in_states_mentioned(x, y)\n",
    "    x = located_based_on_neighbours(x, ne_index, places_and_poi)\n",
    "    x = locate_based_on_population(x)\n",
    "    x = located_based_on_neighbours(x, ne_index, places_and_poi)\n",
    "    x = remove_mere_state_names(x)\n",
    "    x = remove_empty_geo_point(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-botswana",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "lovely-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from vega_datasets import data as vg_data\n",
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "passive-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_points(place_name_geocoding):\n",
    "    name, lat, long = [], [], []\n",
    "    for i in place_name_geocoding:\n",
    "        name.append(i)\n",
    "        lat.append(place_name_geocoding[i][0].lat)\n",
    "        long.append(place_name_geocoding[i][0].long)\n",
    "    df = pd.DataFrame(list(zip(name, lat, long)), columns =['Name', 'Lat', \"Long\"])\n",
    "    states = alt.topo_feature(vg_data.us_10m.url, feature='states')\n",
    "    gdf = gpd.read_file('https://raw.githubusercontent.com/python-visualization/folium/master/tests/us-states.json', driver='GeoJSON')\n",
    "    gdf = gdf[gdf.id=='CA']\n",
    "    \n",
    "    background = alt.Chart(states).mark_geoshape(\n",
    "        fill='lightgray',\n",
    "        stroke='white'\n",
    "    ).project('albersUsa').properties(\n",
    "        width=900,\n",
    "        height=1000\n",
    "    )\n",
    "    \n",
    "    base = alt.Chart(gdf).mark_geoshape(\n",
    "    stroke='black', \n",
    "    fill='lightgray').properties(\n",
    "        width=700,\n",
    "        height=1000\n",
    "    )\n",
    "\n",
    "    points = alt.Chart(df).mark_circle().encode(\n",
    "        longitude='Long:Q',\n",
    "        latitude='Lat:Q',\n",
    "        color=alt.Color('Name:N', legend=alt.Legend(symbolLimit=0)),\n",
    "        size=alt.value(50),\n",
    "        tooltip='Name'\n",
    "    )\n",
    "\n",
    "    return base + points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-fiction",
   "metadata": {},
   "source": [
    "### Test page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fiscal-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_url = data[\"2\"][\"article_url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "declared-pilot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.nationalgeographic.com/travel/article/must-see-stops-destinations-california-route-1'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"2\"][\"article_url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "mobile-essex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 139 ms, sys: 4.87 ms, total: 144 ms\n",
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "%time text, links = text_from_html(page_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "regulated-embassy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 s, sys: 312 ms, total: 13.4 s\n",
      "Wall time: 2.47 s\n"
     ]
    }
   ],
   "source": [
    "%time places_and_poi_processed, places_and_poi, ne_index = get_ne(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time lookup_results = existence_check(places_and_poi_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "later-concentrate",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading formatted geocoded file...\n",
      "CPU times: user 897 ms, sys: 9.82 s, total: 10.7 s\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%time places_geocoded = unify_lookup_results(lookup_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "indoor-going",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 330 µs, sys: 151 µs, total: 481 µs\n",
      "Wall time: 483 µs\n"
     ]
    }
   ],
   "source": [
    "%time place_name_geocoding = disambiguate(places_geocoded, ne_index, places_and_poi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "concerned-blind",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in place_name_geocoding if len(place_name_geocoding[i]) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "living-fantasy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-86c3f7614dc54befae7c4cab47bb0bde\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-86c3f7614dc54befae7c4cab47bb0bde\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-86c3f7614dc54befae7c4cab47bb0bde\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"data\": {\"name\": \"data-014f206b4098cd56928331ca8a368c99\"}, \"mark\": {\"type\": \"geoshape\", \"fill\": \"lightgray\", \"stroke\": \"black\"}, \"height\": 1000, \"width\": 700}, {\"data\": {\"name\": \"data-12bd7c43ed9f265b42402ea3b98e4b4a\"}, \"mark\": \"circle\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Name\", \"legend\": {\"symbolLimit\": 0}}, \"latitude\": {\"field\": \"Lat\", \"type\": \"quantitative\"}, \"longitude\": {\"field\": \"Long\", \"type\": \"quantitative\"}, \"size\": {\"value\": 50}, \"tooltip\": {\"type\": \"nominal\", \"field\": \"Name\"}}}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-014f206b4098cd56928331ca8a368c99\": [{\"id\": \"CA\", \"name\": \"California\", \"type\": \"Feature\", \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-123.233256, 42.006186], [-122.378853, 42.011663], [-121.037003, 41.995232], [-120.001861, 41.995232], [-119.996384, 40.264519], [-120.001861, 38.999346], [-118.71478, 38.101128], [-117.498899, 37.21934], [-116.540435, 36.501861], [-115.85034, 35.970598], [-114.634459, 35.00118], [-114.634459, 34.87521], [-114.470151, 34.710902], [-114.333228, 34.448009], [-114.136058, 34.305608], [-114.256551, 34.174162], [-114.415382, 34.108438], [-114.535874, 33.933176], [-114.497536, 33.697668], [-114.524921, 33.54979], [-114.727567, 33.40739], [-114.661844, 33.034958], [-114.524921, 33.029481], [-114.470151, 32.843265], [-114.524921, 32.755634], [-114.72209, 32.717295], [-116.04751, 32.624187], [-117.126467, 32.536556], [-117.24696, 32.668003], [-117.252437, 32.876127], [-117.329114, 33.122589], [-117.471515, 33.297851], [-117.7837, 33.538836], [-118.183517, 33.763391], [-118.260194, 33.703145], [-118.413548, 33.741483], [-118.391641, 33.840068], [-118.566903, 34.042715], [-118.802411, 33.998899], [-119.218659, 34.146777], [-119.278905, 34.26727], [-119.558229, 34.415147], [-119.875891, 34.40967], [-120.138784, 34.475393], [-120.472878, 34.448009], [-120.64814, 34.579455], [-120.609801, 34.858779], [-120.670048, 34.902595], [-120.631709, 35.099764], [-120.894602, 35.247642], [-120.905556, 35.450289], [-121.004141, 35.461243], [-121.168449, 35.636505], [-121.283465, 35.674843], [-121.332757, 35.784382], [-121.716143, 36.195153], [-121.896882, 36.315645], [-121.935221, 36.638785], [-121.858544, 36.6114], [-121.787344, 36.803093], [-121.929744, 36.978355], [-122.105006, 36.956447], [-122.335038, 37.115279], [-122.417192, 37.241248], [-122.400761, 37.361741], [-122.515777, 37.520572], [-122.515777, 37.783465], [-122.329561, 37.783465], [-122.406238, 38.15042], [-122.488392, 38.112082], [-122.504823, 37.931343], [-122.701993, 37.893004], [-122.937501, 38.029928], [-122.97584, 38.265436], [-123.129194, 38.451652], [-123.331841, 38.566668], [-123.44138, 38.698114], [-123.737134, 38.95553], [-123.687842, 39.032208], [-123.824765, 39.366301], [-123.764519, 39.552517], [-123.85215, 39.831841], [-124.109566, 40.105688], [-124.361506, 40.259042], [-124.410798, 40.439781], [-124.158859, 40.877937], [-124.109566, 41.025814], [-124.158859, 41.14083], [-124.065751, 41.442061], [-124.147905, 41.715908], [-124.257444, 41.781632], [-124.213628, 42.000709], [-123.233256, 42.006186]]]}}], \"data-12bd7c43ed9f265b42402ea3b98e4b4a\": [{\"Name\": \"Santa Ynez\", \"Lat\": 34.61443, \"Long\": -120.07987}, {\"Name\": \"Avila Beach\", \"Lat\": 35.17998, \"Long\": -120.73184}, {\"Name\": \"Crystal Cove State Park\", \"Lat\": 33.57058, \"Long\": -117.81089}, {\"Name\": \"Orange County\", \"Lat\": 33.67691, \"Long\": -117.77617}, {\"Name\": \"Long Beach Convention and Entertainment Center\", \"Lat\": 33.76474, \"Long\": -118.18923}, {\"Name\": \"Los Trancos Creek\", \"Lat\": 37.41383, \"Long\": -122.19163}, {\"Name\": \"State Street\", \"Lat\": 34.42104, \"Long\": -119.70089}, {\"Name\": \"Huntington Beach\", \"Lat\": 33.6603, \"Long\": -117.99923}, {\"Name\": \"Trout Creek Trail\", \"Lat\": 39.35822, \"Long\": -122.69694}, {\"Name\": \"Pacific\", \"Lat\": 38.76171, \"Long\": -120.50843}, {\"Name\": \"El Paseo\", \"Lat\": 33.72222137451172, \"Long\": -116.3744430541992}, {\"Name\": \"Santa Barbara\", \"Lat\": 34.42083, \"Long\": -119.69819}, {\"Name\": \"Harbor House Inn\", \"Lat\": 34.4099, \"Long\": -119.6936}, {\"Name\": \"Aquarium of the Pacific\", \"Lat\": 33.7635, \"Long\": -118.195}, {\"Name\": \"Hearst Castle\", \"Lat\": 35.68525, \"Long\": -121.1677}, {\"Name\": \"Pismo\", \"Lat\": 35.12109, \"Long\": -120.61323}, {\"Name\": \"Buellton\", \"Lat\": 34.6136, \"Long\": -120.19265}, {\"Name\": \"Arroyo Grande\", \"Lat\": 35.11859, \"Long\": -120.59073}, {\"Name\": \"Long Beach\", \"Lat\": 33.76696, \"Long\": -118.18923}, {\"Name\": \"Pismo Beach\", \"Lat\": 35.14275, \"Long\": -120.64128}, {\"Name\": \"Lompoc\", \"Lat\": 34.59693, \"Long\": -120.56267}, {\"Name\": \"Vandenberg Air Force Base\", \"Lat\": 34.7483, \"Long\": -120.51817}, {\"Name\": \"Newport Beach\", \"Lat\": 33.61891, \"Long\": -117.92895}, {\"Name\": \"Southern California\", \"Lat\": 34.68743, \"Long\": -116.78467}, {\"Name\": \"Dunes Center\", \"Lat\": 34.9723272, \"Long\": -120.57213730000001}, {\"Name\": \"Pea Soup Andersen\\u2019s\", \"Lat\": 34.613735000000005, \"Long\": -120.192777}, {\"Name\": \"Edna Valley\", \"Lat\": 35.247586902065855, \"Long\": -120.64344732448377}, {\"Name\": \"Huntington Pier\", \"Lat\": 33.65473937988281, \"Long\": -118.00450134277344}, {\"Name\": \"Guadalupe-Nipomo Dunes\", \"Lat\": 34.97600173950195, \"Long\": -120.6500015258789}, {\"Name\": \"La Purisima Mission\", \"Lat\": 34.67916488647461, \"Long\": -120.4236145019531}, {\"Name\": \"International Surfing Museum\", \"Lat\": 33.6591796875, \"Long\": -118.0008010864258}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualize_points(place_name_geocoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-nutrition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-metabolism",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-sight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-quantum",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-moldova",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
