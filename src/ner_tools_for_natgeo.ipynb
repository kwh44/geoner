{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "literary-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html2text\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "controlling-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_html(url: str): #data_folder_html_file_id):\n",
    "    \"\"\"\n",
    "    Given url of the webpage returns the readable text from the page\n",
    "    \"\"\"\n",
    "    #road_trip = open(\"../data/\" + str(data_folder_html_file_id) + \".html\", \"r\").read()\n",
    "    road_trip = requests.get(url).text\n",
    "    soup = BeautifulSoup(road_trip)\n",
    "    links = []\n",
    "    for link in soup.findAll('a'):\n",
    "        http_link = link.get('href')\n",
    "        if \"nationalgeographic\" in http_link:\n",
    "            continue\n",
    "        if not http_link.startswith(\"http\"):\n",
    "            continue\n",
    "        if len(http_link) > 5:\n",
    "            links.append(http_link)\n",
    "    \n",
    "    text_maker = html2text.HTML2Text()\n",
    "    text_maker.ignore_links = True\n",
    "    text_maker.ignore_images = True\n",
    "    road_trip_processed = text_maker.handle(road_trip)\n",
    "    \n",
    "    start = \"ShareTweetEmail\"\n",
    "    index = road_trip_processed.find(start)\n",
    "    road_trip_processed = road_trip_processed[index + len(start) + 1:]\n",
    "    \n",
    "    end = road_trip_processed.find(\"ShareTweetEmail\")\n",
    "    road_trip_processed = road_trip_processed[:end]\n",
    "    road_trip_processed = road_trip_processed.replace(\"\\n\\n\", \". \")\n",
    "    for i in [\"\\n\", \"[\", \"]\", \"*\", \"#\", \"_\", \"\\\\'\", '\\\\']:\n",
    "        road_trip_processed = road_trip_processed.replace(i, \" \")\n",
    "    road_trip_processed = road_trip_processed.replace('>', \", \")\n",
    "    for i in range(4): # 4 seems enough\n",
    "        road_trip_processed = road_trip_processed.replace(\"  \", \" \")\n",
    "    road_trip_processed = road_trip_processed.replace(\"..\", \".\")\n",
    "               \n",
    "    return road_trip_processed.strip(), links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "changed-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_and_places() -> dict:\n",
    "    with open('/home/geoner/data/articles.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    for i in data:\n",
    "        text, _ = text_from_html(data[i][\"article_url\"])\n",
    "        data[i][\"text\"] = text      \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "exciting-arrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 s, sys: 8.9 ms, total: 1.01 s\n",
      "Wall time: 9.9 s\n"
     ]
    }
   ],
   "source": [
    "%time data = get_text_and_places()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-doctrine",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "charged-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "happy-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_pred = dict()\n",
    "ntlk_target_entities = [\"GPE\", \"LOCATION\", \"FACILITY\", \"ORGANIZATION\"]\n",
    "\n",
    "for i in data:\n",
    "    i_th_data_sample_predictions = []\n",
    "    \n",
    "    for sent in nltk.sent_tokenize(data[i][\"text\"]):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            \n",
    "            if hasattr(chunk, 'label') and chunk.label() in ntlk_target_entities:\n",
    "                extracted_place = \" \".join(word[0] for word in chunk.leaves())\n",
    "                i_th_data_sample_predictions.append(extracted_place)\n",
    "    nltk_pred[i] = i_th_data_sample_predictions\n",
    "ntlk_articles_recall = []\n",
    "for i in data:\n",
    "    article_correctly_extracted_entities_number = 0\n",
    "    for entity in data[i][\"places_and_poi\"]:\n",
    "        if entity in nltk_pred[i]:\n",
    "            article_correctly_extracted_entities_number += 1\n",
    "    ntlk_articles_recall.append(article_correctly_extracted_entities_number / len(data[i][\"places_and_poi\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "unlimited-regard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5251821487691053"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ntlk_articles_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-track",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "paperback-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fantastic-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "pressed-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ne(txt):\n",
    "    ne_types = [\"GPE\", \"FAC\", \"LOC\", \"ORG\"]\n",
    "    ne = nlp(txt).ents\n",
    "    target_ne = [str(ne[i]).strip() for i in range(len(ne)) if ne[i].label_ in ne_types]\n",
    "    for i in range(len(target_ne)):\n",
    "        if target_ne[i].startswith(\"the\"):\n",
    "            target_ne[i] = target_ne[i][4:]\n",
    "        if target_ne[i].endswith(','):\n",
    "            target_ne[i] = target_ne[i][:-1]\n",
    "        target_ne[i] = target_ne[i].strip()\n",
    "    return target_ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "sudden-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_pred = dict()\n",
    "for i in data:\n",
    "    i_th_data_sample_predictions = get_ne(data[i][\"text\"])\n",
    "    spacy_pred[i] = i_th_data_sample_predictions\n",
    "spacy_articles_recall = []\n",
    "for i in data:\n",
    "    article_correctly_extracted_entities_number = 0\n",
    "    for entity in data[i][\"places_and_poi\"]:\n",
    "        if entity in spacy_pred[i]:\n",
    "            article_correctly_extracted_entities_number += 1\n",
    "    spacy_articles_recall.append(article_correctly_extracted_entities_number / len(data[i][\"places_and_poi\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "advanced-breakdown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9575572615790007"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(spacy_articles_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-first",
   "metadata": {},
   "source": [
    "## Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stopped-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "appointed-mainstream",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-15 17:55:07,321 loading file /root/.flair/models/ner-english/4f4cdab26f24cb98b732b389e6cebc646c36f54cfd6e0b7d3b90b25656e4262f.8baa8ae8795f4df80b28e7f7b61d788ecbb057d1dc85aacb316f1bd02837a4a4\n"
     ]
    }
   ],
   "source": [
    "tagger = SequenceTagger.load('flair/ner-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "according-queue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flair_ne(txt: str):\n",
    "    sentence = Sentence(txt)\n",
    "    tagger.predict(sentence)\n",
    "    target_ne = [entity.to_plain_string() for entity in sentence.get_spans() if entity.tag != \"PER\"]\n",
    "    for i in range(len(target_ne)):\n",
    "        if target_ne[i].endswith(\"â€™\"):\n",
    "            target_ne[i] = target_ne[i][:-1]\n",
    "    return target_ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "optimum-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_pred = dict()\n",
    "for i in data:\n",
    "    i_th_data_sample_predictions = get_flair_ne(data[i][\"text\"])\n",
    "    flair_pred[i] = i_th_data_sample_predictions\n",
    "flair_articles_recall = []\n",
    "for i in data:\n",
    "    article_correctly_extracted_entities_number = 0\n",
    "    for entity in data[i][\"places_and_poi\"]:\n",
    "        if entity in flair_pred[i]:\n",
    "            article_correctly_extracted_entities_number += 1\n",
    "    flair_articles_recall.append(article_correctly_extracted_entities_number / len(data[i][\"places_and_poi\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "honey-waste",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8971442969269056"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(flair_articles_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-planet",
   "metadata": {},
   "source": [
    "## StanfordNER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-remove",
   "metadata": {},
   "source": [
    "Running stanford-ner server on localhost:8878\n",
    "/home/stanfordner/stanford-ner-2020-11-17# java -mx1000m -cp stanford-ner-4.2.0.jar edu.stanford.nlp.ie.NERServer -loadClassifier classifiers/[classifier] -port 8878 -outputFormat inlineXML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "supreme-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "african-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = ner.SocketNER(host='localhost', port=8878)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cosmetic-establishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stanford_ne(txt):\n",
    "    ne_dict = tagger.get_entities(txt)\n",
    "    ne = ne_dict[\"LOCATION\"] + ne_dict[\"ORGANIZATION\"]\n",
    "    return ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "tutorial-trader",
   "metadata": {},
   "outputs": [],
   "source": [
    "stanford_pred = dict()\n",
    "for i in data:\n",
    "    i_th_data_sample_predictions = get_stanford_ne(data[i][\"text\"])\n",
    "    stanford_pred[i] = i_th_data_sample_predictions\n",
    "stanford_articles_recall = []\n",
    "for i in data:\n",
    "    article_correctly_extracted_entities_number = 0\n",
    "    for entity in data[i][\"places_and_poi\"]:\n",
    "        if entity in stanford_pred[i]:\n",
    "            article_correctly_extracted_entities_number += 1\n",
    "    stanford_articles_recall.append(article_correctly_extracted_entities_number / len(data[i][\"places_and_poi\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "southern-movie",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6565522118782988"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# english.all.3class.distsim.crf.ser.gz\n",
    "np.mean(stanford_articles_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "special-american",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7034879050096442"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# english.conll.4class.distsim.crf.ser.gz\n",
    "np.mean(stanford_articles_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "indoor-market",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6244530147791016"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# english.muc.7class.distsim.crf.ser.gz \n",
    "np.mean(stanford_articles_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-stomach",
   "metadata": {},
   "source": [
    "## AllenNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "heavy-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp_models import pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "artificial-service",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lerc is not a registered model.\n",
      "visual-entailment is not a registered model.\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/allenai/allennlp-models/blob/main/allennlp_models/modelcards/tagging-fine-grained-transformer-crf-tagger.json\n",
    "predictor = pretrained.load_predictor(\"tagging-fine-grained-transformer-crf-tagger\")\n",
    "# BILUO scheme is used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "spoken-mounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_allennlp_ne(txt: str):\n",
    "    allen_result = predictor.predict(txt)\n",
    "    ents = []\n",
    "    for word, tag in zip(allen_result[\"words\"], allen_result[\"tags\"]):\n",
    "        if tag != \"O\":\n",
    "            ent_position, ent_type = tag.split(\"-\")\n",
    "            \n",
    "            if ent_type not in [\"LOC\", \"GPE\", \"ORG\", \"FAC\", \"PRODUCT\", \"WORK_OF_ART\"]:\n",
    "                continue\n",
    "            if ent_position == \"U\":\n",
    "                ents.append(word)\n",
    "            else:\n",
    "                if ent_position == \"B\":\n",
    "                    w = word\n",
    "                elif ent_position == \"I\":\n",
    "                    w += \" \" + word\n",
    "                elif ent_position == \"L\":\n",
    "                    w += \" \" + word\n",
    "                    ents.append(w)\n",
    "    for i in range(len(ents)):\n",
    "        if ents[i].startswith(\"the\"):\n",
    "            ents[i] = ents[i][4:]\n",
    "        if ents[i].endswith(\"'s\"):\n",
    "            ents[i] = ents[i][:-2]\n",
    "        if ents[i].endswith(','):\n",
    "            ents[i] = ents[i][:-1]    \n",
    "        ents[i] = ents[i].strip()\n",
    "        ents[i] = ents[i].replace(\" 's\", \"'s\")\n",
    "    return ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "russian-fence",
   "metadata": {},
   "outputs": [],
   "source": [
    "allennlp_pred = dict()\n",
    "for i in data:\n",
    "    i_th_data_sample_predictions = get_allennlp_ne(data[i][\"text\"])\n",
    "    allennlp_pred[i] = i_th_data_sample_predictions\n",
    "allennlp_articles_recall = []\n",
    "for i in data:\n",
    "    article_correctly_extracted_entities_number = 0\n",
    "    for entity in data[i][\"places_and_poi\"]:\n",
    "        if entity in allennlp_pred[i]:\n",
    "            article_correctly_extracted_entities_number += 1\n",
    "    allennlp_articles_recall.append(article_correctly_extracted_entities_number / len(data[i][\"places_and_poi\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "prescription-transport",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8018399273834056"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(allennlp_articles_recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
