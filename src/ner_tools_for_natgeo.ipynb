{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "literary-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html2text\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "controlling-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_html(url: str): #data_folder_html_file_id):\n",
    "    \"\"\"\n",
    "    Given url of the webpage returns the readable text from the page\n",
    "    \"\"\"\n",
    "    #road_trip = open(\"../data/\" + str(data_folder_html_file_id) + \".html\", \"r\").read()\n",
    "    road_trip = requests.get(url).text\n",
    "    soup = BeautifulSoup(road_trip)\n",
    "    links = []\n",
    "    for link in soup.findAll('a'):\n",
    "        http_link = link.get('href')\n",
    "        if \"nationalgeographic\" in http_link:\n",
    "            continue\n",
    "        if not http_link.startswith(\"http\"):\n",
    "            continue\n",
    "        if len(http_link) > 5:\n",
    "            links.append(http_link)\n",
    "    \n",
    "    text_maker = html2text.HTML2Text()\n",
    "    text_maker.ignore_links = True\n",
    "    text_maker.ignore_images = True\n",
    "    road_trip_processed = text_maker.handle(road_trip)\n",
    "    \n",
    "    start = \"ShareTweetEmail\"\n",
    "    index = road_trip_processed.find(start)\n",
    "    road_trip_processed = road_trip_processed[index + len(start) + 1:]\n",
    "    \n",
    "    end = road_trip_processed.find(\"ShareTweetEmail\")\n",
    "    road_trip_processed = road_trip_processed[:end]\n",
    "    road_trip_processed = road_trip_processed.replace(\"\\n\\n\", \". \")\n",
    "    for i in [\"\\n\", \"[\", \"]\", \"*\", \"#\", \"_\", \"\\\\'\", '\\\\']:\n",
    "        road_trip_processed = road_trip_processed.replace(i, \" \")\n",
    "    road_trip_processed = road_trip_processed.replace('>', \", \")\n",
    "    for i in range(4): # 4 seems enough\n",
    "        road_trip_processed = road_trip_processed.replace(\"  \", \" \")\n",
    "    road_trip_processed = road_trip_processed.replace(\"..\", \".\")\n",
    "               \n",
    "    return road_trip_processed.strip(), links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "changed-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_and_places() -> dict:\n",
    "    with open('/home/geoner/data/articles.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    for i in data:\n",
    "        text, _ = text_from_html(data[i][\"article_url\"])\n",
    "        data[i][\"text\"] = text      \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "exciting-arrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.12 s, sys: 7.8 ms, total: 1.13 s\n",
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%time data = get_text_and_places()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-doctrine",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "happy-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_pred = dict()\n",
    "ntlk_target_entities = [\"GPE\", \"LOCATION\", \"FACILITY\", \"ORGANIZATION\"]\n",
    "\n",
    "for i in data:\n",
    "    i_th_data_sample_predictions = []\n",
    "    \n",
    "    for sent in nltk.sent_tokenize(data[i][\"text\"]):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            \n",
    "            if hasattr(chunk, 'label') and chunk.label() in ntlk_target_entities:\n",
    "                extracted_place = \" \".join(word[0] for word in chunk.leaves())\n",
    "                i_th_data_sample_predictions.append(extracted_place)\n",
    "    nltk_pred[i] = i_th_data_sample_predictions\n",
    "ntlk_articles_recall = []\n",
    "for i in data:\n",
    "    article_correctly_extracted_entities_number = 0\n",
    "    for entity in data[i][\"places_and_poi\"]:\n",
    "        if entity in nltk_pred[i]:\n",
    "            article_correctly_extracted_entities_number += 1\n",
    "    ntlk_articles_recall.append(article_correctly_extracted_entities_number / len(data[i][\"places_and_poi\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "unlimited-regard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.526400097487054"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ntlk_articles_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-track",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "paperback-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fantastic-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "pressed-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ne(txt):\n",
    "    ne_types = [\"GPE\", \"FAC\", \"LOC\", \"ORG\"]\n",
    "    ne = nlp(txt).ents\n",
    "    target_ne = [str(ne[i]).strip() for i in range(len(ne)) if ne[i].label_ in ne_types]\n",
    "    for i in range(len(target_ne)):\n",
    "        if target_ne[i].startswith(\"the\") or target_ne[i].startswith(\"The\"):\n",
    "            target_ne[i] = target_ne[i][4:]\n",
    "        if target_ne[i].endswith('.') or target_ne[i].endswith(','):\n",
    "            target_ne[i] = target_ne[i][:-1]\n",
    "        target_ne[i] = target_ne[i].strip()\n",
    "    return target_ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "sudden-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_pred = dict()\n",
    "for i in data:\n",
    "    i_th_data_sample_predictions = get_ne(data[i][\"text\"])\n",
    "    spacy_pred[i] = i_th_data_sample_predictions\n",
    "spacy_articles_recall = []\n",
    "for i in data:\n",
    "    article_correctly_extracted_entities_number = 0\n",
    "    for entity in data[i][\"places_and_poi\"]:\n",
    "        if entity in spacy_pred[i]:\n",
    "            article_correctly_extracted_entities_number += 1\n",
    "    spacy_articles_recall.append(article_correctly_extracted_entities_number / len(data[i][\"places_and_poi\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "advanced-breakdown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9505920971138362"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(spacy_articles_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-first",
   "metadata": {},
   "source": [
    "## Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import SequenceTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = SequenceTagger.load('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-wheat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-gasoline",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
